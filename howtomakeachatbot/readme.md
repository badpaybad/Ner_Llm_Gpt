# overview

User send some text 
System response text 

We use stacks: Python, Mongodb, Javascript, Html, css

# how we do system generate text to user

base keywords:

                - KB: knowledge base 
                - RAG: retrieval-augmented generation
                - LLM: lagger language model
                - DB vector: Database eg: mongodb, to store text vector 
                - User prompt: text from user
                - System prompt: text provide along side with User prompt
                - Assitance prompt: text generated by LLM
                - LLM Context: whole history User,System, Assitance prompt of an conversation 
                - Embedding: how to get text vector with semantic

base workflow:

                User prompt -> call to system use KB RAG to get System prompt -> feed System prompt to LLM -> Assitance prompt -> system process Assitance prompt -> feed User propmt to LLM -> Assitance prompt -> display text to user

## how we build KB, RAG 

text vector base on https://huggingface.co/Viet-Mistral/Vistral-7B-Chat

                embeding_RAG/main.py

mongodb to store vector

## Embebding and LLM model 

use LLM to generate text base on https://huggingface.co/Viet-Mistral/Vistral-7B-Chat 

                mistralvn/Vistral-7B-Chat
                mistralvn/main.py

## BE (Backend) Expose API to FE (Frontend)

                Python: https://www.w3schools.com/python/
                Fastapi https://fastapi.tiangolo.com/tutorial/
                Mongodb python: https://www.w3schools.com/python/python_mongodb_getstarted.asp 
                Docker db mongo: https://hub.docker.com/_/mongo
                Tool connect mongodb: https://www.mongodb.com/products/tools/compass 
                or
                C# asp.net core

## FE web

                Html, js, css

## FE mobi app

                Flutter


## dockercompose db

https://www.docker.com/products/docker-desktop/

docker compose -f "docker-compose-db.yml" -p "db" up -d

                services:
                      mongodb6:
                        # user: root
                        #https://hub.docker.com/_/mongo
                        image: mongo:6
                        restart: always
                        ports:
                            - 27017:27017
                        volumes:
                            - /home/dunp/docker/mongo6:/data/db
                        environment:
                            - MONGO_INITDB_ROOT_USERNAME=YourName
                            - MONGO_INITDB_ROOT_PASSWORD=YourPass@Somesecret


# ollama docker run
                
gpu https://github.com/ollama/ollama/blob/main/docs/gpu.md

                sudo apt-get update
                sudo apt-get install -y nvidia-driver-<version>
                sudo apt-get install -y docker.io
                sudo systemctl start docker
                sudo systemctl enable docker

                # Install the NVIDIA Container Toolkit
                sudo mkdir -p /etc/systemd/system/docker.service.d
                sudo tee /etc/systemd/system/docker.service.d/override.conf <<EOF
                [Service]
                ExecStart=
                ExecStart=/usr/bin/dockerd --host=fd:// --add-runtime=nvidia=/usr/bin/nvidia-container-runtime
                EOF

                sudo systemctl daemon-reload
                sudo systemctl restart docker

                docker run --gpus all ollama/ollama-gpu ollama run

                The --gpus all flag tells Docker to use all available GPUs. You can specify particular GPUs if needed (e.g., --gpus '"device=0,1"').

                docker pull ollama/ollama:latest
                docker run -d -p 15000:11434 ollama/ollama

                docker exec -it {id container}> ollama pull mistral
                docker exec -it f455d69e6edcd67678b8924c44eebe1b936ec295a0ef2af7a2051abe7cb57e69 ollama pull mistral

https://ollama.com/library

                docker exec -it f455d69e6edcd67678b8924c44eebe1b936ec295a0ef2af7a2051abe7cb57e69 ollama pull gemma2
                docker exec -it f455d69e6edcd67678b8924c44eebe1b936ec295a0ef2af7a2051abe7cb57e69 ollama pull gemma2:27b
# api usage

https://www.postman.com/bstraehle/workspace/generative-ai-llm-rest-apis/documentation/7643177-2ea8088c-43df-440a-b6de-4a84ac3fa60c

                curl --location 'http://127.0.0.:15000/api/generate' \
                --header 'Content-Type: application/json' \
                --data '{
                    "model": "mistral",
                    "prompt": "What is the meaning of life?",
                    "raw": true,
                    "stream": false
                }'

https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings

                curl http://localhost:11434/api/embed -d '{
                "model": "mistral",
                "input": "Why is the sky blue?"
                }'1